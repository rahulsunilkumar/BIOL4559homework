{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ead983",
   "metadata": {},
   "source": [
    "# Lab 5\n",
    "\n",
    "## Pre class checklist\n",
    "Have you...\n",
    "\n",
    "* Watched the video on Matplotlib (https://www.youtube.com/watch?v=qErBw-R2Ybk)? Not necessary but useful!\n",
    "* Downloaded the dataset (check out the labs page and it will be linked there)?\n",
    "* Downloaded VSCode?\n",
    "\n",
    "From this lab on you dont need to create new branches, you can straight up save in your main branch on github. The submission will take place via canvas, you will have to upload your `.ipynb` notebook directly to canvas.\n",
    "\n",
    "**How you will be graded**\n",
    "\n",
    "This activity is out of 10 points. You will be graded on three things: completion, clear documentation, and correctness of your process.\n",
    "\n",
    "* *Completion (4pts)*: This is solely whether you attempted and completed all portions of the computational assignment.\n",
    "* *Documentation (4pts)*: Write comments above your code, describing what it does to demonstrate that you understand what the code is doing.\n",
    "* *Correctness (2pts)*: Does your code run and produce the correct output?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87e832",
   "metadata": {},
   "source": [
    "# Playground!\n",
    "\n",
    "In this section you will be given a set of tasks to complete on your own, and you will have to figure out how to do them using the resources provided (videos, documentation, etc). This way you can learn at your own pace and also get used to looking up documentation and learning how to use new tools on your own.\n",
    "\n",
    "Note: We will have periodic check-ins through the class, just to make sure everyone atleast finishes a part of the activity before moving on!\n",
    "\n",
    "The very first task is to import the required packages, this lab we will import a few datasets, create networks out of them, plot some statistics and finally fit a regression thorugh the plots.\n",
    "\n",
    "Try importing the required libraries below (dont forget you can use the `as` keyword to give them a shorter alias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39856829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afc8b5ff",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "#### 1.1 Loading a single dataset\n",
    "\n",
    "For this lab we will work with `interactome datasets`. The interactome network of protein-protein interactions captures the structure of molecular machinery and gives rise to a bewildering degree of life complexity. A critical property of interactomes is the resilience to protein failures as the breakdown of proteins affects the exchange of any biological information between proteins in a cell and may lead to cell death or disease.\n",
    "\n",
    "By studying interactomes from 1,840 species across the tree of life, we find that evolution leads to more resilient interactomes, providing evidence for a longstanding hypothesis that interactomes evolve favoring robustness against protein failures. We find that a highly resilient interactome has a beneficial impact on the organism's survival in complex, variable, and competitive habitats.\n",
    "\n",
    "Answers to the question of PPI network evolution have implications for informing biology and answering numerous questions at the forefront of medicine, from interpreting genome-wide association data to drug target identification and drug discovery.\n",
    "\n",
    "These interactions are stored as a directed edge list (with the first column indicating the source and the second indicating the target) within a few `.txt` files, with each file corresponding to a specific organism.\n",
    "\n",
    "We can read a `.txt` file using `pl.read_csv()` as well, we will just have to define the separator (what is seperating two values) and the column names (since the `.txt` dont have them already):\n",
    "```python\n",
    "df = pl.read_csv(\"data_to_read.txt\", separator = \" \", has_header=False, new_columns=[\"source\", \"target\"])\n",
    "# Each column is seperated by a space hence the seperator is just a space.\n",
    "# We want to tell python that this file doesnt already have headers (The column names) within the data, so we use the has_headers argument.\n",
    "# new_columns defines the new column headers we want the DataFrame to have.\n",
    "```\n",
    "\n",
    "In the code cell below try loading one `.txt` in a variable named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185062b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ae5497",
   "metadata": {},
   "source": [
    "#### 1.2 Creating a dictionary of datasets\n",
    "\n",
    "This time we are going to work with a few different interactome networks, so we need to load all these datasets into Python.  \n",
    "\n",
    "One way would be to create `df1`, `df2`, `df3`, and so on as separate variables. But as your code grows, this quickly becomes messy.  \n",
    "\n",
    "Instead, we can use a **dictionary**. If you dont remember them from previous labs then think of a dictionary like a labeled storage box:  \n",
    "- Each **key** is the label (like `\"human\"` or `\"yeast\"`)  \n",
    "- Each **value** is what’s inside (in our case, a DataFrame).  \n",
    "\n",
    "Example:\n",
    "```python\n",
    "sample_dict = {\"key1\": 1, \"key2\": 210}  # key:value pairs\n",
    "\n",
    "# Adding a new entry:\n",
    "sample_dict[\"key3\"] = 234\n",
    "print(sample_dict)\n",
    "```\n",
    "If we were to do the same with dataframes, it would look a bit like this,\n",
    "\n",
    "```python\n",
    "df_dict = {}\n",
    "files_to_read = [\"filepath1\", \"filepath2\", \"filepath3\"]\n",
    "for i in range(len(files_to_read)):\n",
    "    df_tmp = pl.read_csv(files_to_read[i], seperator = \" \", has_header=False, \n",
    "        new_columns=[\"source\", \"target\"])\n",
    "\n",
    "    df_dict[files_to_read[i]] = df_tmp\n",
    "```\n",
    "\n",
    "There might be other ways too! Above code is just one example. You can try writing your own code or use my example (make sure to comment all the code!).\n",
    "\n",
    "Print out the dictionary after you create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb6298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d4b22b",
   "metadata": {},
   "source": [
    "## 2. Creating graphs from our data!\n",
    "\n",
    "#### 2.1 Creating a single graph\n",
    "\n",
    "From previous labs we know how to convert a single dataframe to a networkx graph. Till now we did it manually like this,\n",
    "```python\n",
    "G = nx.Graph() # We create an empty graph\n",
    "\n",
    "# We add nodes\n",
    "G.add_nodes_from(df['source'])\n",
    "G.add_nodes_from(df['target'])\n",
    "\n",
    "# Then we add edges\n",
    "G.add_edges_from(df['source', 'target'].iter_rows())\n",
    "\n",
    "```\n",
    "This step-by-step approach shows what’s happening under the hood.\n",
    "\n",
    "But in practice, networkx gives us a shortcut, it allows us to directly convert a dataframe to a graph using the `nx.from_pandas_edgelist()` function (although the functions asks for a pandas dataframe, it will also accept a polars dataframe). \n",
    "\n",
    "Example usage:\n",
    "```python\n",
    "G = nx.from_pandas_edgelist(df, source = <source_node>, target = <target_node>)\n",
    "# Here we first input the dataframe we want to use, the source argument specifies which column is being used for source node and target argument specifies the column name for target node.\n",
    "nx.draw(G, node_size = 10, node_color = \"gray\", alpha = 0.1)\n",
    "```\n",
    "\n",
    "Try using this function to create and draw a graph from one of your dataframes (try extracting the dataframe from you dictionary of dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64adc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "774ca95c",
   "metadata": {},
   "source": [
    "#### 2.2 Dictionary of Graphs\n",
    "\n",
    "Just like we created a dictionary of dataframes, we can also create a dictionary of graph objects. \n",
    "\n",
    "Try creating a dictionary of Graphs for all your dataframes.\n",
    "\n",
    "Hint: Refactor the code you used to create the dataframe dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7cd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "591500a4",
   "metadata": {},
   "source": [
    "Now we can use this dictionary to draw your graphs using `nx.draw()`, you can try and play around with your visualization like making node size proportional to its degree! Below is the code to do that.\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots(figsize = (12,12), nrows = 3, ncols = 3)\n",
    "\n",
    "for idx, i in enumerate(files_to_read):\n",
    "    G_tmp = G_dict[i]\n",
    "    n_dict = dict(G_tmp.degree)\n",
    "    nx.draw(G_tmp, nodelist=n_dict.keys(), node_size=[v * 10 for v in n_dict.values()], node_color = \"gray\", alpha = 0.1, ax=ax[int(idx // 3), int(idx % 3)])\n",
    "    ax[int(idx // 3), int(idx % 3)].set_title(i)\n",
    "\n",
    "plt.tight_layout()\n",
    "```\n",
    "\n",
    "Try to figure out what each line is doing in this code and add those comments! You can also try to play around with the arguments on your own, maybe change the node size based on each node's clustering coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830da9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1bde30b",
   "metadata": {},
   "source": [
    "## 3. Plotting network statistics\n",
    "\n",
    "#### 3.1 Frequency vs Degree plot\n",
    "\n",
    "Lets start with a simple frequency plot for our network degrees. We can already create a list with degrees of each node from our Graph. We just have to find a way to plot this list as a graph. For this we can use the `matplotlib` library.\n",
    "\n",
    "To plot a simple frequency plot from a list, we can do the following,\n",
    "\n",
    "```python\n",
    "lst = [3,3,3,3,1,1,1,2,2,2,2,2,4,5,5,5,5]\n",
    "vals, freq = np.unique(lst, return_counts = True)\n",
    "# np.unique looks for each unique element and returns two arrays, one with the unique elements and second with their frequency.\n",
    "\n",
    "print(f\"Unique values: {vals}\")\n",
    "print(f\"Frequencies: {freq}\")\n",
    "\n",
    "plt.scatter(vals, freq, marker = 'o')\n",
    "# plt.scatter just puts a point on a graph corresponding to each value of vals as x-axis and freq as y-axis.\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "We can adapt this code to print our degree distribution!\n",
    "```python\n",
    "degrees = [deg for _, deg in G.degree()] # For any graph G\n",
    "\n",
    "vals, freq = np.unique(degrees, return_counts=True)\n",
    "\n",
    "plt.scatter(vals, freq, marker=\"o\")\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Degree distribution (yeast)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Try it out on your own, use a `for` loop to iterate over your graph dictionary and print the degree distributions for each!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3dc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa6eb78a",
   "metadata": {},
   "source": [
    "#### 3.2 Converting the graph to log scale\n",
    "\n",
    "In many biological networks, the degree distribution is very skewed:\n",
    "- Lots of nodes have very low degree\n",
    "- A few nodes have very high degree  \n",
    "\n",
    "On a **linear scale**, this makes the plot look squished to the left.  \n",
    "A good trick is to switch to a **logarithmic scale** for the axes.\n",
    "\n",
    "Matplotlib allows you to play around with the scaling of each axis in your graph. By default the scaling is linear, but you can use `plt.xscale('log')` and `plt.yscale('log')` to change the scaling into a logarithimic. \n",
    "\n",
    "Use the code from your previous code cell and just add these two commands before `plt.show()`, do you see any change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57568be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60a66b79",
   "metadata": {},
   "source": [
    "#### 3.3 Regression\n",
    "\n",
    "The final step is to fit a regression line to our degree distribution in **log-log space**.  \n",
    "\n",
    "Why?  \n",
    "If the data points roughly fall on a straight line, it suggests that the degree distribution follows a **power-law**, which is a hallmark of scale-free networks (common in biology!).\n",
    "\n",
    "\n",
    "Your task:  \n",
    "- Fit a regression line on the log-log degree distribution.  \n",
    "- Overlay that line on your scatterplot.  \n",
    "\n",
    "Hint: `np.polyfit()` might come in handy, use to on the LOG of x and y. This gives us the slope and intercept for $log(y) = m * log(x) + b$ which translates to $y = e^b x^m$, dont forget you can also consult the internet or call me over!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183ee06",
   "metadata": {},
   "source": [
    "# Extra Credit!\n",
    "\n",
    "Can you use plot all the graphs/networks within the same figure, as a grid?\n",
    "\n",
    "Matplotlib allows you to do this, you will get **1 EXTRA POINT** if you successfully do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f99316",
   "metadata": {},
   "source": [
    "**After the completion of this lab**\n",
    "1. List the people with whom you worked on this lab.\n",
    "2. Go to Canvas, and submit your `lab5.ipynb` under the assignments tab (No need to deal with GitHub, Yay!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biol4559",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
